# TUMCUD - Total Utility Maxmimization model for Cutting of Under-segmented Data

Open your VM and download the jupyter notebook file

```
wget --no-check-certificate https://raw.githubusercontent.com/ekapolc/nlp_course/master/HW1/Word_Tokenizer_Lab.ipynb
```

Submit the completed notebook file on MyCourseVille
